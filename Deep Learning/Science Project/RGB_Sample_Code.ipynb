{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB #Sample code\n",
    "\n",
    "## Project Summary\n",
    "The total economic benefits of global ecosystem services such as pollination and water purification are estimated to be $125 trillion USD per year[1]. Hence, with the rapid biodiversity decline around the globe[2], the efforts to understand and protect our invaluable ecosystem is needed now more than ever. Additionally, the use of capture-recapture - among other traditional methods for ecological research and conservation - can be extremely costly and labour-intensive[3], hence demanding for their automation. An improvement in ecological research techniques is especially needed when the resources for such efforts are often limited[4].\n",
    "\n",
    "In particular, the advancement in drone technology, hyperspectral imaging and artificial intelligence may allow for a cost-effective automation of the scat detection and collection process.\n",
    "\n",
    "To test the feasibility of such an idea, around 2000 spectral readings were collected. The data was then used to test the ability of hyperspectral imaging, in combination with a neural network, to differentiate the reflectance spectra of scat samples, in comparison to those of other non-scat elements.\n",
    "\n",
    "The results showed a significant improvement in the average prediction accuracy when hyperspectral data were used instead of RGB data (z-score of 4.45). Hence, the result signifies the potential of hyperspectral drones to identify scat samples in the wild.\n",
    "\n",
    "By collaborating with researchers or organisations with hyperspectral drones, field tests may be carried out in the future to validate the dronesâ€™ abilities to automate the detection and collection of these ecological samples.\n",
    "\n",
    "Further information:",
    " https://www.googlesciencefair.com/projects/2018/c34a66ad0bac164d4a30b480d0c62aa97c428c8dd29747e4a04488e8be64cc74\n",
    "\n",
    "## ML code\n",
    "\n",
    "### Click [here](https://www.youtube.com/watch?v=9uzh3SIXSJc&feature=youtu.be) for the youtube tutorial \n",
    "\n",
    "### Click [here](InteractiveDisplay.ipynb) for an interactive display of the neural network result\n",
    "\n",
    "A neural network is a computational tool that has to ability to easily learn complex patterns from a large quantity of data for applications such as classification and feature imitation.\n",
    "\n",
    "A neural network consists of input nodes and subsequent nodes that sum the value of nodes before it, with each node value multiplied by a scalar. Initially, the scalar values are randomly assigned. But a partial derivative can determine the influence of each node on the final output. The values of the scalars are then updated based on the influence/partial-derivative values and the sizes of the classification errors. Over time, this gradient descent process would improve the classification accuracy. With sufficient training data, neural networks can then be used to identify scat samples in the wild based on their reflectance spectra.[33]\n",
    "\n",
    "Train your own neural network by selecting the \"Kernel\" at the top and click \"Restart Kernel and Run All Cells...\"\n",
    "\n",
    "<img src=\"../Image/Instruction.PNG\" width=\"800\" />\n",
    "\n",
    "\n",
    "### Dead Kernal\n",
    "\n",
    "<img src=\"../Image/DeadKernal.PNG\" width=\"30\"  />\n",
    "\n",
    "\n",
    "___This symbol would appear if the Kernal becomes dead. This is because the mybinder.org juypter notebook hosting service is free, where the RAM memory is limited.  .___ \n",
    "___Select \"Restart & Run All\" as you did at the start.___\n",
    "\n",
    "## Setting up\n",
    "\n",
    "#### 1. Specifying the RGB Network specific variables\n",
    "Other than the variables in the next cell, the rest of the code is common for both an RGB and hyperspectral network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = '../training_rgb_sample'\n",
    "data_path =\"../Training data/RGB.csv\"\n",
    "n_o_input = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Importing Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "#### 3. Importing data and splitting it into label and training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "\n",
    "data.head()\n",
    "\n",
    "target_fields ='Class code'\n",
    "data = data.drop([\"Class\",\"Sample\",\"Session.Sample\"],axis=1)\n",
    "features0, targets0 = data.drop(target_fields, axis=1), data[target_fields]\n",
    "features, targets  = np.array(features0) , np.array(targets0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Function to generate a non-repeating list of random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_no_repeat(lower,upper,length):\n",
    "    assert length<=(upper-lower)+1\n",
    "    output = []\n",
    "    while len(output)<length:\n",
    "        x = random.randint(lower,upper)\n",
    "        if not(x in output):\n",
    "            output.append(x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Pick spectral data from random sessions for testing and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_length = 1\n",
    "test_length = 1\n",
    "n_o_sample = 47\n",
    "size_of_a_sample = 7\n",
    "\n",
    "#Pick random labels for validation & test set\n",
    "val_labels = []\n",
    "test_labels = [] \n",
    "for num in range(n_o_sample):\n",
    "    rand_upper = size_of_a_sample-1\n",
    "    rand_len = test_length+validation_length\n",
    "    ran_list = random_no_repeat(0,rand_upper,rand_len)\n",
    "    \n",
    "    rand_valid = np.array(ran_list[0:validation_length])\n",
    "    val_labels.extend(n_o_sample*rand_valid+num)\n",
    "    \n",
    "    rand_t_start = validation_length\n",
    "    rand_t_end = validation_length+test_length\n",
    "    rand_test= np.array(ran_list[rand_t_start:rand_t_end])\n",
    "    test_labels.extend(n_o_sample*rand_test+num)\n",
    "\n",
    "#Labels for the training set by removing those allocated to test & validation\n",
    "train_labels = list(\n",
    "    set([x for x in range(n_o_sample*size_of_a_sample)])\n",
    "    - set(val_labels)\n",
    "    - set(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 1\n",
    "Check that the random session index for each sample is picked correctly, where the labels of the validation and test set should be same for a given sample number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[141] [282]\n",
      "[1] [1]\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "print(num) #Sample number\n",
    "print(n_o_sample*rand_valid+num,n_o_sample*rand_test+num) #Index of sample from random trials\n",
    "print(targets[n_o_sample*rand_valid+num],targets[n_o_sample*rand_test+num]) #Lables of random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. The samples are ordered in related groups. Randomizing the data would remove biases from the sample's related order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(train_labels)\n",
    "shuffle(val_labels)\n",
    "shuffle(test_labels)\n",
    "train_x, train_y = features[train_labels] ,targets[train_labels]\n",
    "val_x , val_y = features[val_labels] , targets[val_labels]\n",
    "test_x, test_y = features[test_labels] ,targets[test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 2\n",
    "Check that labels are in the same order (disable the shuffle function from step 6 first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 8 1 1 1 3 6 3 3 4 5 1 1 3 5 2 3 8 2 2 5 4 4 1 2 3 4 2 2 2 3 7 1 5 3 2 5\n",
      " 4 3 3 2 3 2 2 6 7 2]\n",
      "[2 2 3 2 4 1 1 4 4 3 2 1 7 4 3 3 8 3 2 1 2 8 3 5 5 6 5 3 2 3 2 3 3 2 2 1 5\n",
      " 1 1 2 5 3 7 4 6 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_y)\n",
    "print(val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring the networks\n",
    "#### 7. The 1st layer of the hyperspectral network\n",
    "For the hyperspectral data set, an additional layer was added to the start of the network topology to reduce the dataâ€™s complexity. Traditionally, the first hidden layer looks for simpler features, in this case, the individual absorption features.[34] Looking for correlation at an individual wavelength level, however, requires much computational cost. But as the absorption features exist as clusters of neighbouring wavelengths, we can use a convolution network like approach to convey the sequential and incremental relationship of wavelength inputs. The prioritization of relationships among neighbouring inputs would reduce the computational cost. This convolution network like structures was, however, built from scratch, by stacking smaller dense layers together. This was to avoid the spatial invariance, whose generalisation in absorption features characteristics would disregard their potential variance across the spectrum.\n",
    "\n",
    "For image classification, a neural network should be able to recognise the same features or objects regardless of location. For example, the classification network should be able to recognise an orange regardless of its position on an image, whether it be at the top corner, the bottom corner or anywhere else. The use of the same image feature extraction process regardless of location is called translational invariance.[34]\n",
    "\n",
    "Instead, absorption features in hyperspectral reflectance data may vary in characteristics in a different location of the spectrum. The shape of an absorption feature in the infrared region of the spectrum may differ from the one in the visible region. Hence, the convolutional-like neural network is customised to remove the translational invariance feature. Different feature extraction processes would then be trained for interpreting absorption features from a different region of the spectrum.\n",
    "\n",
    "\n",
    "<img src=\"../Image/Invariance.jpg\" width=\"600\" />\n",
    "\n",
    "\n",
    "A leaky RELU (Rectified linear unit) was also used in both networks as the activation function. This ensured the required non-linearity for problem-solving without the high computational cost of functions like softmax. It also prevented the exploding gradient in backpropagation that is associated with the conventional RELU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperspectral(input_,n_o_input, keep_prob, filter_width = 1, stride_size =1, relu_alpha = 0.2):\n",
    "    n_o_strides = int((n_o_input-filter_width)/stride_size) +1  #round down\n",
    "   \n",
    "    Hyper_layer = []\n",
    "    \n",
    "    def dense(input_,start,keep_prob, filter_width, stride_size, relu_alpha):\n",
    "        nn_input = tf.slice(input_,[0,start],[-1,filter_width])\n",
    "        \n",
    "        dropout1 = tf.nn.dropout(nn_input, keep_prob)\n",
    "        dense1 = tf.layers.dense(dropout1, 1)\n",
    "        relu1 = tf.maximum(relu_alpha * dense1, dense1)        \n",
    "        return relu1\n",
    "    \n",
    "    for step in range(n_o_strides):\n",
    "        start = step*stride_size\n",
    "        output = dense(input_,start,keep_prob, filter_width, stride_size, relu_alpha)\n",
    "        Hyper_layer.append(output)\n",
    "    \n",
    "    if (n_o_input-filter_width)%stride_size>0:\n",
    "        start = n_o_input-filter_width\n",
    "        output = dense(input_,start,keep_prob, filter_width, stride_size, relu_alpha)\n",
    "        Hyper_layer.append(output)\n",
    "        \n",
    "    Hyper_l_stacked = tf.concat(Hyper_layer,1)\n",
    "    \n",
    "    print(\"Hyper_l_stacked\",Hyper_l_stacked)\n",
    "    return Hyper_l_stacked , n_o_strides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. The remaining layers which are common for both the hyperspectral and RGB network\n",
    "Consisting of 2 layers, the remaining structures for the neural networks of the RGB and hyperspectral data set are similar. Both outputs were then consisted of 8 nodes, for the 8 total sub-classes of dropping and abiotic elements. For the droppings, the classes were segregated by the 5 types of excrement and the animal of origin. For the abiotic element, the classes were instead segregated by 3 material types. [See table from the methodology section of the report] For the 2 networks, the sizes of the hidden layer prior to the outputs were then both calculated by multiplying the size of the prior layer with a â…” ratio, a structure that is common in neural network development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Classifier(input_,n_o_class,n_o_input, keep_prob,relu_alpha = 0.2):\n",
    "    print(\"n_o_input\",n_o_input)\n",
    "    if n_o_input == 3:\n",
    "        is_RGB = True\n",
    "    elif n_o_input == 571:\n",
    "        is_RGB = False\n",
    "    else:\n",
    "        raise ValueError('A very specific bad thing happened.'+str(n_o_input))\n",
    "    \n",
    "    if is_RGB:\n",
    "        dense0 = tf.layers.dense(input_, 3)    \n",
    "        relu0 = tf.maximum(relu_alpha * dense0, dense0)\n",
    "        first_layer_out = tf.nn.dropout(relu0, keep_prob)\n",
    "    else:\n",
    "        first_layer_out,n_o_input= hyperspectral(input_,n_o_input, keep_prob, filter_width = 30, stride_size =1, relu_alpha = 0.2)\n",
    "\n",
    "    hidden_size = n_o_input*2/3\n",
    "    hidden_nodes = int(hidden_size)+1 # rounding\n",
    "    print(\"hidden size:\",str(hidden_nodes))\n",
    "    \n",
    "    \n",
    "    dense1 = tf.layers.dense(first_layer_out, hidden_nodes)    \n",
    "    relu1 = tf.maximum(relu_alpha * dense1, dense1)\n",
    "    dropout1 = tf.nn.dropout(relu1, keep_prob)\n",
    "    \n",
    "    \n",
    "    class_logits = tf.layers.dense(dropout1, n_o_class)    \n",
    "    \n",
    "    return class_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up for training\n",
    "#### 9. Function to format neural network input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_indv(x,n_o_class):    \n",
    "    output = np.zeros(n_o_class)\n",
    "    output[x-1]=1\n",
    "    return output\n",
    "\n",
    "def one_hot_encode(x,n_o_class):\n",
    "    output = []\n",
    "    for y in x:\n",
    "        output.append(one_hot_indv(y,n_o_class))\n",
    "        \n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Function to split data into smaller batch for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y,batch_size=10):\n",
    "    n_batches = len(x)//batch_size\n",
    "    \n",
    "    for ii in range(0, n_batches*batch_size, batch_size):\n",
    "        # If we're not on the last batch, grab data with size batch_size\n",
    "        if ii != (n_batches-1)*batch_size:\n",
    "            X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] \n",
    "        # On the last batch, grab the rest of the data\n",
    "        else:\n",
    "            X, Y = x[ii:], y[ii:]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Function to define loss value for training\n",
    "Inspired by the aggregate loss scoring system from a GAN semi-supervised network, an aggregated scoring system was structured to calculate the networkâ€™s loss value for training. This averaged the loss between the main class classification and the subclass classification; the main classes being manure vs non-manure, and the subclasses being the specific animal class or material type. And this aggregated scoring system aimed to teach the network the relationship between the groups of subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_loss(input_,target_,m_class, n_class,n_o_input, keep_prob,relu_alpha = 0.2,sub_scaling = 1):\n",
    "    \n",
    "    n_o_class = m_class+n_class\n",
    "    \n",
    "    #raw output\n",
    "    logits= Classifier(input_,n_o_class,n_o_input, keep_prob,relu_alpha = 0.2)\n",
    "    subclass_softmax = tf.nn.softmax(logits)\n",
    "    \n",
    "    #Reduce outputs from 8 subclasses to 2 main classes\n",
    "    n_class_logit, m_class_logit = tf.split(logits, [n_class, m_class], 1)\n",
    "    m_class_logit1 =tf.reduce_sum(m_class_logit,1, keepdims =True) \n",
    "    n_class_logit1 =tf.reduce_sum(n_class_logit,1, keepdims =True) \n",
    "    main_class_logits = tf.concat([n_class_logit1, m_class_logit1], 1)\n",
    "    main_class_softmax = tf.nn.softmax(main_class_logits)\n",
    "    \n",
    "    #Reduce labels from 8 subclasses to 2 main classes\n",
    "    n_class_label, m_class_label = tf.split(target_, [n_class, m_class], 1)\n",
    "    m_class_label1 =tf.reduce_sum(m_class_label,1, keepdims =True) \n",
    "    n_class_label1 =tf.reduce_sum(n_class_label,1, keepdims =True) \n",
    "    main_class_labels = tf.concat([n_class_label1, m_class_label1], 1)\n",
    "\n",
    "    #Aggregated cost\n",
    "    sub_class_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=target_))\n",
    "    main_class_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=main_class_logits, labels=main_class_labels))\n",
    "    total_cost = sub_class_cost + sub_scaling*main_class_cost\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(total_cost)\n",
    "    \n",
    "    #Accuracy value\n",
    "    subclass_accuracy = tf.equal(tf.argmax(logits, 1), tf.argmax(target_, 1))\n",
    "    subclass_accuracy =  tf.reduce_mean(tf.cast(subclass_accuracy, tf.float32), name='accuracy') #raw score \n",
    "    \n",
    "    main_class_accuracy = tf.equal(tf.argmax(main_class_logits, 1), tf.argmax(main_class_labels, 1))\n",
    "    main_class_accuracy = tf.reduce_mean(tf.cast(main_class_accuracy, tf.float32), name='accuracy') #raw score \n",
    "    \n",
    "    confidence_sub_class =  tf.reduce_sum(tf.multiply(subclass_softmax,target_),1)\n",
    "    confidence_main_class =  tf.reduce_sum(tf.multiply(main_class_softmax,main_class_labels),1)\n",
    "    \n",
    "    return optimizer,total_cost,subclass_softmax,main_class_softmax,subclass_accuracy,main_class_accuracy,confidence_sub_class,confidence_main_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "#### 12. Defining the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_o_input 3\n",
      "hidden size: 3\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_class = 3\n",
    "m_class = 5\n",
    "n_o_class = m_class+n_class\n",
    "input_ = tf.placeholder(tf.float32,  [None,n_o_input],name = 'x')\n",
    "target_ = tf.placeholder(tf.float32,[None,n_o_class],name='y')\n",
    "keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "epochs = 10\n",
    "keep_probability = 0.95\n",
    "sub_scaling = 1 \n",
    "optimizer,total_cost,subclass_softmax,main_class_softmax,subclass_accuracy,main_class_accuracy,confidence_sub_class,confidence_main_class =model_loss(input_,target_,m_class, n_class,n_o_input, keep_prob,relu_alpha = 0.2,sub_scaling = sub_scaling) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Doing the actual training\n",
    "The neural network was modified based on the classification accuracy for the validation set. These modifications are made to find the \"optimal\" structure with the best classification performance.\n",
    "- Prediction accuracy represents that % of readings in each batch whose subclass is correctly classified\n",
    "- Prediction confidence represents the cross-entropy score. In statistics, a prediction at a 95% confidence would mean that the prediction would be correct for 95% of the time where such confidence value was given. While the networkâ€™s classification confidence doesnâ€™t directly translate to a statistical confidence, the confidence value of a well trained neural network does exist as a good indication of future prediction accuracy. [35]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Validation Epoch  1 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.51403, Subclass Accuracy: 0.30000, Subclass Confidence: 0.12722\n",
      "Validation Epoch  2 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.51352, Subclass Accuracy: 0.20000, Subclass Confidence: 0.12700\n",
      "Validation Epoch  3 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.53852, Subclass Accuracy: 0.20000, Subclass Confidence: 0.12826\n",
      "Validation Epoch  4 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.52371, Subclass Accuracy: 0.40000, Subclass Confidence: 0.12857\n",
      "Validation Epoch  5 - Main Class Accuracy: 1.00000, , Main Class Confidence 0.63881, Subclass Accuracy: 0.30000, Subclass Confidence: 0.13701\n",
      "Validation Epoch  6 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.53037, Subclass Accuracy: 0.30000, Subclass Confidence: 0.13106\n",
      "Validation Epoch  7 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.53290, Subclass Accuracy: 0.10000, Subclass Confidence: 0.13097\n",
      "Validation Epoch  8 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.53501, Subclass Accuracy: 0.20000, Subclass Confidence: 0.13248\n",
      "Validation Epoch  9 - Main Class Accuracy: 0.60000, , Main Class Confidence 0.53701, Subclass Accuracy: 0.20000, Subclass Confidence: 0.13185\n",
      "Validation Epoch 10 - Main Class Accuracy: 0.70000, , Main Class Confidence 0.57702, Subclass Accuracy: 0.20000, Subclass Confidence: 0.13419\n"
     ]
    }
   ],
   "source": [
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, label_pre_one_hot in get_batches(train_x, train_y,batch_size = 10):\n",
    "            batch_labels = one_hot_encode(label_pre_one_hot,n_o_class)\n",
    "            \n",
    "            optimizer_p= sess.run([optimizer], feed_dict = {input_:batch_features,target_:batch_labels,keep_prob:keep_probability})\n",
    "\n",
    "            batch_i += 1 \n",
    "        print('Validation Epoch {:>2} - '.format(epoch + 1), end='')\n",
    "        \n",
    "        random_index = random_no_repeat(0,len(val_x)-1,10)\n",
    "        valid_labels = one_hot_encode(val_y[random_index],n_o_class) \n",
    "        subclass_accuracy_p,main_class_accuracy_p,confidence_sub_class_p,confidence_main_class_p= sess.run([subclass_accuracy,main_class_accuracy,confidence_sub_class,confidence_main_class], feed_dict = {input_:val_x[random_index],target_:valid_labels,keep_prob:1})\n",
    "        print(\"Main Class Accuracy: {:.5f}, , Main Class Confidence {:.5f}, Subclass Accuracy: {:.5f}, Subclass Confidence: {:.5f}\".format(np.mean(main_class_accuracy_p),np.mean(confidence_main_class_p),np.mean(subclass_accuracy_p),np.mean(confidence_sub_class_p)))\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "#### 13. Testing the neural network\n",
    "When the modifications made were no longer improving the validation accuracy, we were satisfied that we had achieved the \"optimal\" structure. The network was then tested with an unseen set of spectral data to confirm network's performance. \n",
    "In the previous validation phase, modifications were made based on the network's previous performance, where some of the improvements may be specific to the validation data set. Instead, in the testing phase, the use of unseen data would produce accuracy values that were representative of real-world performance. [36]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "INFO:tensorflow:Restoring parameters from ../training_rgb_sample\n",
      "Sample 1, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69270, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14202\n",
      "Sample 2, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69271, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14202\n",
      "Sample 3, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69257, Subclass Accuracy: 1.00000, Subclass Confidence: 0.14934\n",
      "Sample 4, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69267, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14201\n",
      "Sample 5, Class 4,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30690, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11856\n",
      "Sample 6, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69264, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13231\n",
      "Sample 7, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69275, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13294\n",
      "Sample 8, Class 4,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30537, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11931\n",
      "Sample 9, Class 4,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30708, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11832\n",
      "Sample 10, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69293, Subclass Accuracy: 1.00000, Subclass Confidence: 0.15113\n",
      "Sample 11, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69266, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14205\n",
      "Sample 12, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69297, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13242\n",
      "Sample 13, Class 7,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30727, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11088\n",
      "Sample 14, Class 4,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30576, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11898\n",
      "Sample 15, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69256, Subclass Accuracy: 1.00000, Subclass Confidence: 0.14886\n",
      "Sample 16, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69363, Subclass Accuracy: 1.00000, Subclass Confidence: 0.15388\n",
      "Sample 17, Class 8,Main Class Accuracy: 0.00000, Main Class Confidence: 0.31248, Subclass Accuracy: 0.00000, Subclass Confidence: 0.10316\n",
      "Sample 18, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69344, Subclass Accuracy: 1.00000, Subclass Confidence: 0.15948\n",
      "Sample 19, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69331, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14185\n",
      "Sample 20, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69283, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13270\n",
      "Sample 21, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69273, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14200\n",
      "Sample 22, Class 8,Main Class Accuracy: 0.00000, Main Class Confidence: 0.31495, Subclass Accuracy: 0.00000, Subclass Confidence: 0.10035\n",
      "Sample 23, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69318, Subclass Accuracy: 1.00000, Subclass Confidence: 0.15123\n",
      "Sample 24, Class 5,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30680, Subclass Accuracy: 0.00000, Subclass Confidence: 0.12042\n",
      "Sample 25, Class 5,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30690, Subclass Accuracy: 0.00000, Subclass Confidence: 0.12051\n",
      "Sample 26, Class 6,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30686, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11492\n",
      "Sample 27, Class 5,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30719, Subclass Accuracy: 0.00000, Subclass Confidence: 0.12061\n",
      "Sample 28, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69277, Subclass Accuracy: 1.00000, Subclass Confidence: 0.14983\n",
      "Sample 29, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69326, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14178\n",
      "Sample 30, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69261, Subclass Accuracy: 1.00000, Subclass Confidence: 0.14901\n",
      "Sample 31, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69326, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14179\n",
      "Sample 32, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69261, Subclass Accuracy: 1.00000, Subclass Confidence: 0.15015\n",
      "Sample 33, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69262, Subclass Accuracy: 1.00000, Subclass Confidence: 0.14907\n",
      "Sample 34, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69283, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14197\n",
      "Sample 35, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69273, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14203\n",
      "Sample 36, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69282, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13251\n",
      "Sample 37, Class 5,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30652, Subclass Accuracy: 0.00000, Subclass Confidence: 0.12033\n",
      "Sample 38, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69283, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13199\n",
      "Sample 39, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69268, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13322\n",
      "Sample 40, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69274, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14202\n",
      "Sample 41, Class 5,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30696, Subclass Accuracy: 0.00000, Subclass Confidence: 0.12057\n",
      "Sample 42, Class 3,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69307, Subclass Accuracy: 1.00000, Subclass Confidence: 0.15231\n",
      "Sample 43, Class 7,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30527, Subclass Accuracy: 0.00000, Subclass Confidence: 0.10947\n",
      "Sample 44, Class 4,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30519, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11905\n",
      "Sample 45, Class 6,Main Class Accuracy: 0.00000, Main Class Confidence: 0.30714, Subclass Accuracy: 0.00000, Subclass Confidence: 0.11451\n",
      "Sample 46, Class 1,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69335, Subclass Accuracy: 0.00000, Subclass Confidence: 0.13100\n",
      "Sample 47, Class 2,Main Class Accuracy: 1.00000, Main Class Confidence: 0.69281, Subclass Accuracy: 0.00000, Subclass Confidence: 0.14200\n"
     ]
    }
   ],
   "source": [
    "keep_probability = 1\n",
    "print('Testing...')\n",
    "with tf.Session() as sess:\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        batch_i = 1\n",
    "        for batch_features, label_pre_one_hot in get_batches(val_x, val_y,batch_size = 1):# to be changed\n",
    "            batch_labels = one_hot_encode(label_pre_one_hot,n_o_class)      \n",
    "            \n",
    "            subclass_accuracy_p,main_class_accuracy_p,confidence_sub_class_p,confidence_main_class_p= sess.run([subclass_accuracy,main_class_accuracy,confidence_sub_class,confidence_main_class], feed_dict = {input_:batch_features,target_:batch_labels,keep_prob:1})\n",
    "            print('Sample {}, Class {},'.format(batch_i,label_pre_one_hot[0]), end='')\n",
    "            print(\"Main Class Accuracy: {:.5f}, Main Class Confidence: {:.5f}, Subclass Accuracy: {:.5f}, Subclass Confidence: {:.5f}\".format(np.mean(main_class_accuracy_p),np.mean(confidence_main_class_p),np.mean(subclass_accuracy_p),np.mean(confidence_sub_class_p)))\n",
    "\n",
    "            batch_i += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Plantsocieties.cnps.org. (2018). Overview - Ecosystem Services - Native Plant Conservation Campaign. [online] Available at: http://plantsocieties.cnps.org/index.php/ecoss-main/overview [Accessed 12 Dec. 2018].\n",
    "2. Nature.com. (2018). Home : Nature Status. [online] Available at: https://www.nature.com/scitable/knowledge/library/causes-and-consequences-of-biodiversity-declines-16132475 [Accessed 12 Dec. 2018].\n",
    "3. J. SWANSON, B. and O'NEIL, E. (2018). Using Track-plate Footprints in Fisher Mark Recapture Population Estimation. [online] Available at: https://www.jstor.org/stable/40730960 [Accessed 12 Dec. 2018].\n",
    "4. Cell.com. (2018). Cell Press: Cell Press. [online] Available at: http://www.cell.com/current-biology/fulltext/S0960-9822(11)01263-2 [Accessed 12 Dec. 2018].\n",
    "33. Medium. (2018). How Do Neural Networks Work? â€“ Machine Intelligence Report â€“ Medium. [online] Available at: https://medium.com/machine-intelligence-report/how-do-neural-networks-work-57d1ab5337ce [Accessed 13 Dec. 2018].\n",
    "34. Ime.usp.br. (2018). Introduction to Convolutional Neural Networks - Julio M. Otuyama. [online] Available at: https://www.ime.usp.br/~otuyama/academic/cnn/index.html [Accessed 13 Dec. 2018].\n",
    "35. Cse.unsw.edu.au. (2018). What is cross-entropy, and why use it?. [online] Available at: http://www.cse.unsw.edu.au/~billw/cs9444/crossentropy.html [Accessed 13 Dec. 2018].\n",
    "36. Towards Data Science. (2018). About Train, Validation and Test Sets in Machine Learning. [online] Available at: https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7 [Accessed 13 Dec. 2018]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
